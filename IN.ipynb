{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "548d38af-2960-46a0-8d6a-627ba42adc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import yaml\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Dataset, Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from pytorch_lightning import LightningModule\n",
    "from torch_geometric.nn import aggr\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "import torch.optim as optim\n",
    "from torch_scatter import scatter_add\n",
    "from torch import Tensor\n",
    "from torch_geometric.nn import MessagePassing\n",
    "\n",
    "from torch_geometric.utils import to_networkx\n",
    "from torch.nn import Sequential as Seq, Linear, ReLU, Sigmoid\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from collections import namedtuple\n",
    "\n",
    "from pytorch_lightning import LightningModule\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29a0be8c-163d-4421-b8e7-eebfc8c8513e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted PyTorch Version: 2.2.1+cu121\n",
      "Formatted CUDA Version: 12.1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "TORCH = torch.__version__\n",
    "CUDA = torch.version.cuda\n",
    "\n",
    "print(f\"Formatted PyTorch Version: {TORCH}\")\n",
    "print(f\"Formatted CUDA Version: {CUDA}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4963f919-9bcd-4e8d-ab51-6039bd85017b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "\n",
    "class GraphDataset(Dataset):\n",
    "    def __init__(self, home_dir, sub_dir):\n",
    "        self.base_path = os.path.join(home_dir, sub_dir)\n",
    "        self.file_names = self._get_file_names()\n",
    "\n",
    "    def _get_file_names(self):\n",
    "        file_names = []\n",
    "        for file_name in os.listdir(self.base_path):\n",
    "            if file_name.endswith('.pyg'):  # Adjust this condition as needed\n",
    "                file_names.append(file_name)\n",
    "        return file_names\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_name = self.file_names[idx]\n",
    "        file_path = os.path.join(self.base_path, file_name)\n",
    "        data = torch.load(file_path)\n",
    "        print(f\"Loaded data from {file_path}: {data}\")\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ea81a82-349d-408d-a110-6b1e216fa63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "home_dir = \"/users/santoshp/standalone_IN_gnn/data/\"\n",
    "test = 'test_set/'\n",
    "train = 'train_set/'\n",
    "val = 'val_set/'\n",
    "\n",
    "# Create dataset instances\n",
    "test_dataset = GraphDataset(home_dir, test)\n",
    "train_dataset = GraphDataset(home_dir, train)\n",
    "val_dataset = GraphDataset(home_dir, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7e328fa-e6b7-4ba6-bcb6-f533728cdb41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data from /users/santoshp/standalone_IN_gnn/data/train_set/event005008281_00.pyg: Data(eta_angle_1=[281060], cluster_x_2=[281060], module_id=[281060], cluster_z_1=[281060], eta_angle_2=[281060], phi=[281060], cluster_eta_2=[281060], norm_x_2=[281060], region=[281060], r=[281060], cluster_r_2=[281060], phi_angle_1=[281060], cluster_x_1=[281060], cluster_y_1=[281060], y=[1464430], z=[281060], hit_id=[281060], norm_y_1=[281060], cluster_r_1=[281060], cluster_phi_2=[281060], norm_z_2=[281060], norm_z_1=[281060], x=[281060], eta=[281060], cluster_eta_1=[281060], phi_angle_2=[281060], cluster_y_2=[281060], cluster_z_2=[281060], norm_x_1=[281060], norm_y_2=[281060], cluster_phi_1=[281060], track_edges=[2, 110033], pdgId=[110033], nhits=[110033], eta_particle=[110033], primary=[110033], particle_id=[110033], redundant_split_edges=[110033], pt=[110033], radius=[110033], config=[1], event_id='005008281', edge_index=[2, 1464430], truth_map=[110033], phi_region_id='0', eta_region_id='0')\n",
      "Data(eta_angle_1=[281060], cluster_x_2=[281060], module_id=[281060], cluster_z_1=[281060], eta_angle_2=[281060], phi=[281060], cluster_eta_2=[281060], norm_x_2=[281060], region=[281060], r=[281060], cluster_r_2=[281060], phi_angle_1=[281060], cluster_x_1=[281060], cluster_y_1=[281060], y=[1464430], z=[281060], hit_id=[281060], norm_y_1=[281060], cluster_r_1=[281060], cluster_phi_2=[281060], norm_z_2=[281060], norm_z_1=[281060], x=[281060], eta=[281060], cluster_eta_1=[281060], phi_angle_2=[281060], cluster_y_2=[281060], cluster_z_2=[281060], norm_x_1=[281060], norm_y_2=[281060], cluster_phi_1=[281060], track_edges=[2, 110033], pdgId=[110033], nhits=[110033], eta_particle=[110033], primary=[110033], particle_id=[110033], redundant_split_edges=[110033], pt=[110033], radius=[110033], config=[1], event_id='005008281', edge_index=[2, 1464430], truth_map=[110033], phi_region_id='0', eta_region_id='0')\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52decc57-0236-46fe-818e-234a80d4496c",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'batch_size': 8, 'shuffle': True, 'num_workers': 0}\n",
    "train_loader = DataLoader(train_dataset,**params)  #batches join graphs instead of splitting them therefore more than train set batches will make 1 batch only \n",
    "test_loader = DataLoader(test_dataset, **params)\n",
    "val_loader = DataLoader(val_dataset, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb1eaeb4-9e52-4b56-9e53-39809deb05d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "208c5ee3-3794-4589-8d94-e7c236578675",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    \"stage\": \"edge_classifier\",\n",
    "    \"model\": \"InteractionGNN2\",\n",
    "    \"input_dir\": \"/users/santoshp/standalone_IN_gnn/data/\",\n",
    "    \"stage_dir\": \"/users/santoshp/standalone_IN_gnn/\",\n",
    "    \"project\": \"Stand_alone\",\n",
    "    \"gpus\": 1,\n",
    "    \"nodes\": 1,\n",
    "    \"data_split\": [10, 5, 5],\n",
    "    \"dataset_class\": \"GraphDataset\",\n",
    "    \"undirected\": False,\n",
    "    \"weighting\": [\n",
    "        {\"weight\": 0.1, \"conditions\": {\"y\": False}},\n",
    "        {\"weight\": 0.0, \"conditions\": {\"y\": True}},\n",
    "        {\n",
    "            \"weight\": 1.0,\n",
    "            \"conditions\": {\n",
    "                \"y\": True,\n",
    "                \"pt\": [1000, float(\"inf\")],\n",
    "                \"nhits\": [3, float(\"inf\")],\n",
    "                \"primary\": True,\n",
    "                \"pdgId\": [\"not_in\", [11, -11]],\n",
    "                \"radius\": [0.0, 260.0],\n",
    "                \"eta_particle\": [-4.0, 4.0],\n",
    "                \"redundant_split_edges\": False,\n",
    "            },\n",
    "        },\n",
    "    ],\n",
    "    \"edge_cut\": 0.5,\n",
    "    \"node_features\": [\n",
    "        \"r\",\n",
    "        \"phi\",\n",
    "        \"z\",\n",
    "        \"eta\",\n",
    "        \"cluster_r_1\",\n",
    "        \"cluster_phi_1\",\n",
    "        \"cluster_z_1\",\n",
    "        \"cluster_eta_1\",\n",
    "        \"cluster_r_2\",\n",
    "        \"cluster_phi_2\",\n",
    "        \"cluster_z_2\",\n",
    "        \"cluster_eta_2\",\n",
    "    ],\n",
    "    \"node_scales\": [\n",
    "        1000.0,\n",
    "        3.14159265359,\n",
    "        1000.0,\n",
    "        1.0,\n",
    "        1000.0,\n",
    "        3.14159265359,\n",
    "        1000.0,\n",
    "        1.0,\n",
    "        1000.0,\n",
    "        3.14159265359,\n",
    "        1000.0,\n",
    "        1.0,\n",
    "    ],\n",
    "    \"edge_features\": [\"radius\"],#[\"dr\", \"dphi\", \"dz\", \"deta\", \"phislope\", \"rphislope\"],\n",
    "    \"hidden\": 128,\n",
    "    \"n_graph_iters\": 8,\n",
    "    \"n_node_encoder_layers\": 3,\n",
    "    \"n_edge_encoder_layers\": 3,\n",
    "    \"n_node_net_layers\": 3,\n",
    "    \"n_edge_net_layers\": 3,\n",
    "    \"n_node_decoder_layers\": 3,\n",
    "    \"n_edge_decoder_layers\": 3,\n",
    "    \"layernorm\": False,\n",
    "    \"output_layer_norm\": False,\n",
    "    \"edge_output_transform_final_layer_norm\": False,\n",
    "    \"batchnorm\": False,\n",
    "    \"output_batch_norm\": False,\n",
    "    \"edge_output_transform_final_batch_norm\": False,\n",
    "    \"bn_track_running_stats\": False,\n",
    "    \"hidden_activation\": \"ReLU\",\n",
    "    \"output_activation\": \"ReLU\",\n",
    "    \"edge_output_transform_final_activation\": None,\n",
    "    \"concat\": True,\n",
    "    \"node_net_recurrent\": False,\n",
    "    \"edge_net_recurrent\": False,\n",
    "    \"in_out_diff_agg\": True,\n",
    "    \"checkpointing\": True,\n",
    "    \"warmup\": 5,\n",
    "    \"lr\": 0.0005,\n",
    "    \"min_lr\": 0.000005,\n",
    "    \"factor\": 0.9,\n",
    "    \"patience\": 15,\n",
    "    \"max_epochs\": 1000,\n",
    "    \"max_training_graph_size\": 2800000,\n",
    "    \"debug\": False,\n",
    "    \"num_workers\": [8, 8, 8],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02ae7559-31f4-4e56-be3b-2a115f6c2db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_mlp(\n",
    "    input_size,\n",
    "    sizes,\n",
    "    hidden_activation=\"ReLU\",\n",
    "    output_activation=None,\n",
    "    layer_norm=False,  # TODO : change name to hidden_layer_norm while ensuring backward compatibility\n",
    "    output_layer_norm=False,\n",
    "    batch_norm=False,  # TODO : change name to hidden_batch_norm while ensuring backward compatibility\n",
    "    output_batch_norm=False,\n",
    "    input_dropout=0,\n",
    "    hidden_dropout=0,\n",
    "    track_running_stats=False,\n",
    "):\n",
    "    \"\"\"Construct an MLP with specified fully-connected layers.\"\"\"\n",
    "    hidden_activation = getattr(nn, hidden_activation)\n",
    "    if output_activation is not None:\n",
    "        output_activation = getattr(nn, output_activation)\n",
    "    layers = []\n",
    "    n_layers = len(sizes)\n",
    "    sizes = [input_size] + sizes\n",
    "    # Hidden layers\n",
    "    for i in range(n_layers - 1):\n",
    "        if i == 0 and input_dropout > 0:\n",
    "            layers.append(nn.Dropout(input_dropout))\n",
    "        layers.append(nn.Linear(sizes[i], sizes[i + 1]))\n",
    "        if layer_norm:  # hidden_layer_norm\n",
    "            layers.append(nn.LayerNorm(sizes[i + 1], elementwise_affine=False))\n",
    "        if batch_norm:  # hidden_batch_norm\n",
    "            layers.append(\n",
    "                nn.BatchNorm1d(\n",
    "                    sizes[i + 1],\n",
    "                    eps=6e-05,\n",
    "                    track_running_stats=track_running_stats,\n",
    "                    affine=True,\n",
    "                )  # TODO : Set BatchNorm and LayerNorm parameters in config file ?\n",
    "            )\n",
    "        layers.append(hidden_activation())\n",
    "        if hidden_dropout > 0:\n",
    "            layers.append(nn.Dropout(hidden_dropout))\n",
    "    # Final layer\n",
    "    layers.append(nn.Linear(sizes[-2], sizes[-1]))\n",
    "    if output_activation is not None:\n",
    "        if output_layer_norm:\n",
    "            layers.append(nn.LayerNorm(sizes[-1], elementwise_affine=False))\n",
    "        if output_batch_norm:\n",
    "            layers.append(\n",
    "                nn.BatchNorm1d(\n",
    "                    sizes[-1],\n",
    "                    eps=6e-05,\n",
    "                    track_running_stats=track_running_stats,\n",
    "                    affine=True,\n",
    "                )  # TODO : Set BatchNorm and LayerNorm parameters in config file ?\n",
    "            )\n",
    "        layers.append(output_activation())\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b04e2132-fa0f-4314-b41b-defb05ad119b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#class InteractionGNN2(EdgeClassifierStage):\n",
    "class InteractionGNN2(LightningModule):\n",
    "\n",
    "    \"\"\"\n",
    "    Interaction Network (L2IT version).\n",
    "    Operates on directed graphs.\n",
    "    Aggregate and reduce (sum) separately incomming and outcoming edges latents.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hparams):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(hparams)\n",
    "\n",
    "        hparams[\"batchnorm\"] = (\n",
    "            False if \"batchnorm\" not in hparams else hparams[\"batchnorm\"]\n",
    "        )\n",
    "        hparams[\"output_batch_norm\"] = hparams.get(\"output_batch_norm\", False)\n",
    "        hparams[\"edge_output_transform_final_batch_norm\"] = hparams.get(\n",
    "            \"edge_output_transform_final_batch_norm\", False\n",
    "        )\n",
    "        hparams[\"edge_output_transform_final_batch_norm\"] = hparams.get(\n",
    "            \"edge_output_transform_final_batch_norm\", False\n",
    "        )\n",
    "        hparams[\"track_running_stats\"] = (\n",
    "            False\n",
    "            if \"track_running_stats\" not in hparams\n",
    "            else hparams[\"track_running_stats\"]\n",
    "        )\n",
    "\n",
    "        # TODO: Add equivalent check and default values for other model parameters ?\n",
    "        # TODO: Use get() method\n",
    "\n",
    "        # Define the dataset to be used, if not using the default\n",
    "        #self.save_hyperparameters(hparams)\n",
    "\n",
    "        # self.setup_layer_sizes()\n",
    "\n",
    "        if hparams[\"concat\"]:\n",
    "            if hparams[\"in_out_diff_agg\"]:\n",
    "                in_node_net = hparams[\"hidden\"] * 4\n",
    "            else:\n",
    "                in_node_net = hparams[\"hidden\"] * 3\n",
    "            in_edge_net = hparams[\"hidden\"] * 6\n",
    "        else:\n",
    "            if hparams[\"in_out_diff_agg\"]:\n",
    "                in_node_net = hparams[\"hidden\"] * 3\n",
    "            else:\n",
    "                in_node_net = hparams[\"hidden\"] * 2\n",
    "            in_edge_net = hparams[\"hidden\"] * 3\n",
    "        # node encoder\n",
    "        self.node_encoder = make_mlp(\n",
    "            input_size=len(hparams[\"node_features\"]),\n",
    "            sizes=[hparams[\"hidden\"]] * hparams[\"n_node_net_layers\"],\n",
    "            output_activation=hparams[\"output_activation\"],\n",
    "            hidden_activation=hparams[\"hidden_activation\"],\n",
    "            layer_norm=hparams[\"layernorm\"],\n",
    "            batch_norm=hparams[\"batchnorm\"],\n",
    "            output_batch_norm=hparams[\"output_batch_norm\"],\n",
    "            track_running_stats=hparams[\"track_running_stats\"],\n",
    "        )\n",
    "     \n",
    "        # edge encoder\n",
    "        if \"edge_features\" in hparams and len(hparams[\"edge_features\"]) != 0:\n",
    "            self.edge_encoder = make_mlp(\n",
    "                input_size=len(hparams[\"edge_features\"]),\n",
    "                sizes=[hparams[\"hidden\"]] * hparams[\"n_edge_net_layers\"],\n",
    "                output_activation=hparams[\"output_activation\"],\n",
    "                hidden_activation=hparams[\"hidden_activation\"],\n",
    "                layer_norm=hparams[\"layernorm\"],\n",
    "                batch_norm=hparams[\"batchnorm\"],\n",
    "                output_batch_norm=hparams[\"output_batch_norm\"],\n",
    "                track_running_stats=hparams[\"track_running_stats\"],\n",
    "            )\n",
    "        else:\n",
    "            self.edge_encoder = make_mlp(\n",
    "                input_size=2 * hparams[\"hidden\"],\n",
    "                sizes=[hparams[\"hidden\"]] * hparams[\"n_edge_net_layers\"],\n",
    "                output_activation=hparams[\"output_activation\"],\n",
    "                hidden_activation=hparams[\"hidden_activation\"],\n",
    "                layer_norm=hparams[\"layernorm\"],\n",
    "                batch_norm=hparams[\"batchnorm\"],\n",
    "                output_batch_norm=hparams[\"output_batch_norm\"],\n",
    "                track_running_stats=hparams[\"track_running_stats\"],\n",
    "            )\n",
    "\n",
    "        # edge network\n",
    "        if hparams[\"edge_net_recurrent\"]:\n",
    "            self.edge_network = make_mlp(\n",
    "                input_size=in_edge_net,\n",
    "                sizes=[hparams[\"hidden\"]] * hparams[\"n_edge_net_layers\"],\n",
    "                output_activation=hparams[\"output_activation\"],\n",
    "                hidden_activation=hparams[\"hidden_activation\"],\n",
    "                layer_norm=hparams[\"layernorm\"],\n",
    "                batch_norm=hparams[\"batchnorm\"],\n",
    "                output_batch_norm=hparams[\"output_batch_norm\"],\n",
    "                track_running_stats=hparams[\"track_running_stats\"],\n",
    "            )\n",
    "        else:\n",
    "            self.edge_network = nn.ModuleList(\n",
    "                [\n",
    "                    make_mlp(\n",
    "                        input_size=in_edge_net,\n",
    "                        sizes=[hparams[\"hidden\"]] * hparams[\"n_edge_net_layers\"],\n",
    "                        output_activation=hparams[\"output_activation\"],\n",
    "                        hidden_activation=hparams[\"hidden_activation\"],\n",
    "                        layer_norm=hparams[\"layernorm\"],\n",
    "                        batch_norm=hparams[\"batchnorm\"],\n",
    "                        output_batch_norm=hparams[\"output_batch_norm\"],\n",
    "                        track_running_stats=hparams[\"track_running_stats\"],\n",
    "                    )\n",
    "                    for i in range(hparams[\"n_graph_iters\"])\n",
    "                ]\n",
    "            )\n",
    "        # node network\n",
    "        if hparams[\"node_net_recurrent\"]:\n",
    "            self.node_network = make_mlp(\n",
    "                input_size=in_node_net,\n",
    "                sizes=[hparams[\"hidden\"]] * hparams[\"n_node_net_layers\"],\n",
    "                output_activation=hparams[\"output_activation\"],\n",
    "                hidden_activation=hparams[\"hidden_activation\"],\n",
    "                layer_norm=hparams[\"layernorm\"],\n",
    "                batch_norm=hparams[\"batchnorm\"],\n",
    "                output_batch_norm=hparams[\"output_batch_norm\"],\n",
    "                track_running_stats=hparams[\"track_running_stats\"],\n",
    "            )\n",
    "        else:\n",
    "            self.node_network = nn.ModuleList(\n",
    "                [\n",
    "                    make_mlp(\n",
    "                        input_size=in_node_net,\n",
    "                        sizes=[hparams[\"hidden\"]] * hparams[\"n_node_net_layers\"],\n",
    "                        output_activation=hparams[\"output_activation\"],\n",
    "                        hidden_activation=hparams[\"hidden_activation\"],\n",
    "                        layer_norm=hparams[\"layernorm\"],\n",
    "                        batch_norm=hparams[\"batchnorm\"],\n",
    "                        output_batch_norm=hparams[\"output_batch_norm\"],\n",
    "                        track_running_stats=hparams[\"track_running_stats\"],\n",
    "                    )\n",
    "                    for i in range(hparams[\"n_graph_iters\"])\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        # edge decoder\n",
    "        self.edge_decoder = make_mlp(\n",
    "            input_size=hparams[\"hidden\"],\n",
    "            sizes=[hparams[\"hidden\"]] * hparams[\"n_edge_decoder_layers\"],\n",
    "            output_activation=hparams[\"output_activation\"],\n",
    "            hidden_activation=hparams[\"hidden_activation\"],\n",
    "            layer_norm=hparams[\"layernorm\"],\n",
    "            batch_norm=hparams[\"batchnorm\"],\n",
    "            output_batch_norm=hparams[\"output_batch_norm\"],\n",
    "            track_running_stats=hparams[\"track_running_stats\"],\n",
    "        )\n",
    "        # edge output transform layer\n",
    "        self.edge_output_transform = make_mlp(\n",
    "            input_size=hparams[\"hidden\"],\n",
    "            sizes=[hparams[\"hidden\"], 1],\n",
    "            output_activation=hparams[\"edge_output_transform_final_activation\"],\n",
    "            hidden_activation=hparams[\"hidden_activation\"],\n",
    "            layer_norm=hparams[\"layernorm\"],\n",
    "            batch_norm=hparams[\"batchnorm\"],\n",
    "            output_batch_norm=hparams[\"edge_output_transform_final_batch_norm\"],\n",
    "            track_running_stats=hparams[\"track_running_stats\"],\n",
    "        )\n",
    "\n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        # hyperparams\n",
    "        # self.hparams = hparams\n",
    "\n",
    "\n",
    "\n",
    "    ###############################\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # Forward pass\n",
    "        output = self(batch)\n",
    "        \n",
    "        # Dummy loss calculation\n",
    "        loss = output.mean()\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.get(\"lr\", 0.001))\n",
    "        return optimizer\n",
    "\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        \"\"\"\n",
    "        Load the training set.\n",
    "        \"\"\"\n",
    "        return DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=4)\n",
    "\n",
    "    def add_edge_features(self, event):\n",
    "        if \"edge_features\" in self.hparams.keys():\n",
    "            assert isinstance(\n",
    "                self.hparams[\"edge_features\"], list\n",
    "            ), \"Edge features must be a list of strings\"\n",
    "            # Create the edge feature tensor with the appropriate shape\n",
    "            num_edges = event.edge_index.shape[1]\n",
    "            num_edge_features = len(self.hparams[\"edge_features\"])\n",
    "            edge_features = torch.randn((num_edges, num_edge_features))  # Example random initialization\n",
    "            event.edge_attr = edge_features\n",
    "        return event\n",
    "\n",
    "    ###########################\n",
    "\n",
    "    def forward(self, batch):\n",
    "        x = torch.stack(\n",
    "            [batch[feature] for feature in self.hparams[\"node_features\"]], dim=-1\n",
    "        ).float()\n",
    "\n",
    "        # Same features on the 3 channels in the STRIP ENDCAP TODO: Process it in previous stage\n",
    "        mask = torch.logical_or(batch.region == 2, batch.region == 6).reshape(-1)\n",
    "        x[mask] = torch.cat([x[mask, 0:4], x[mask, 0:4], x[mask, 0:4]], dim=1)\n",
    "        # print(x[:, 8:12])\n",
    "\n",
    "        if \"edge_features\" in self.hparams and len(self.hparams) != 0:\n",
    "            edge_attr = torch.stack(\n",
    "                [batch[feature] for feature in self.hparams[\"edge_features\"]], dim=-1\n",
    "            ).float()\n",
    "        else:\n",
    "            edge_attr = None\n",
    "\n",
    "        x.requires_grad = True\n",
    "        if edge_attr is not None:\n",
    "            edge_attr.requires_grad = True\n",
    "\n",
    "        # Get src and dst\n",
    "        src, dst = batch.edge_index\n",
    "\n",
    "####################\n",
    "         # Call handle_edge_features function to handle edge features\n",
    "        #handle_edge_features(batch, self.hparams.get(\"edge_features\", []))\n",
    "####################\n",
    "\n",
    "        # Encode nodes and edges features into latent spaces\n",
    "        if self.hparams[\"checkpointing\"]:\n",
    "            x = checkpoint(self.node_encoder, x)\n",
    "            if edge_attr is not None:\n",
    "                e = checkpoint(self.edge_encoder, edge_attr)\n",
    "            else:\n",
    "                e = checkpoint(self.edge_encoder, torch.cat([x[src], x[dst]], dim=-1))\n",
    "        else:\n",
    "            x = self.node_encoder(x)\n",
    "            if edge_attr is not None:\n",
    "                e = self.edge_encoder(edge_attr)\n",
    "            else:\n",
    "                e = self.edge_encoder(torch.cat([x[src], x[dst]], dim=-1))\n",
    "        # Apply dropout\n",
    "        # x = self.dropout(x)\n",
    "        # e = self.dropout(e)\n",
    "\n",
    "        # memorize initial encodings for concatenate in the gnn loop if request\n",
    "        if self.hparams[\"concat\"]:\n",
    "            input_x = x\n",
    "            input_e = e\n",
    "        # Initialize outputs\n",
    "        outputs = []\n",
    "        # Loop over gnn layers\n",
    "        for i in range(self.hparams[\"n_graph_iters\"]):\n",
    "            if self.hparams[\"checkpointing\"]:\n",
    "                if self.hparams[\"concat\"]:\n",
    "                    x = checkpoint(self.concat, x, input_x)\n",
    "                    e = checkpoint(self.concat, e, input_e)\n",
    "                if (\n",
    "                    self.hparams[\"node_net_recurrent\"]\n",
    "                    and self.hparams[\"edge_net_recurrent\"]\n",
    "                ):\n",
    "                    x, e, out = checkpoint(self.message_step, x, e, src, dst)\n",
    "                else:\n",
    "                    x, e, out = checkpoint(self.message_step, x, e, src, dst, i)\n",
    "            else:\n",
    "                if self.hparams[\"concat\"]:\n",
    "                    x = torch.cat([x, input_x], dim=-1)\n",
    "                    e = torch.cat([e, input_e], dim=-1)\n",
    "                if (\n",
    "                    self.hparams[\"node_net_recurrent\"]\n",
    "                    and self.hparams[\"edge_net_recurrent\"]\n",
    "                ):\n",
    "                    x, e, out = self.message_step(x, e, src, dst)\n",
    "                else:\n",
    "                    x, e, out = self.message_step(x, e, src, dst, i)\n",
    "            outputs.append(out)\n",
    "        return outputs[-1].squeeze(-1)\n",
    "\n",
    "    def message_step(self, x, e, src, dst, i=None):\n",
    "        print(\"Shape of e tensor:\", e.size())\n",
    "        print(\"Shape of x[src] tensor:\", x[src].size())\n",
    "        print(\"Shape of x[dst] tensor:\", x[dst].size())\n",
    "        \n",
    "        assert e.shape[0] == x[src].shape[0] #\"Number of rows in e and x[src] must match\"\n",
    "        x_src = x[src].unsqueeze(0)  # Add an extra dimension\n",
    "        x_dst = x[dst].unsqueeze(0)  # Add an extra dimension\n",
    "        e_with_dummy = torch.cat((e, torch.ones_like(e[:, :1])), dim=1)  # Add a dummy feature column\n",
    "\n",
    "\n",
    "        edge_inputs = torch.cat([e, x[src], x[dst]], dim=-1)  # order dst src x ?\n",
    "        if self.hparams[\"edge_net_recurrent\"]:\n",
    "            e_updated = self.edge_network(edge_inputs)\n",
    "        else:\n",
    "            e_updated = self.edge_network[i](edge_inputs)\n",
    "        # Update nodes\n",
    "        edge_messages_from_src = scatter_add(e_updated, dst, dim=0, dim_size=x.shape[0])\n",
    "        edge_messages_from_dst = scatter_add(e_updated, src, dim=0, dim_size=x.shape[0])\n",
    "        if self.hparams[\"in_out_diff_agg\"]:\n",
    "            node_inputs = torch.cat(\n",
    "                [edge_messages_from_src, edge_messages_from_dst, x], dim=-1\n",
    "            )  # to check : the order dst src  x ?\n",
    "        else:\n",
    "            # add message from src and dst ?? # edge_messages = edge_messages_from_src + edge_messages_from_dst\n",
    "            edge_messages = edge_messages_from_src + edge_messages_from_dst\n",
    "            node_inputs = torch.cat([edge_messages, x], dim=-1)\n",
    "        # x_updated = self.dropout(self.node_network[i](node_inputs))\n",
    "        if self.hparams[\"node_net_recurrent\"]:\n",
    "            x_updated = self.node_network(node_inputs)\n",
    "        else:\n",
    "            x_updated = self.node_network[i](node_inputs)\n",
    "\n",
    "        return (\n",
    "            x_updated,\n",
    "            e_updated,\n",
    "            self.edge_output_transform(self.edge_decoder(e_updated)),\n",
    "        )\n",
    "\n",
    "    def concat(self, x, y):\n",
    "        return torch.cat([x, y], dim=-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82c2841f-516f-4006-b44e-51fb793bf4ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InteractionGNN2(\n",
       "  (node_encoder): Sequential(\n",
       "    (0): Linear(in_features=12, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (5): ReLU()\n",
       "  )\n",
       "  (edge_encoder): Sequential(\n",
       "    (0): Linear(in_features=1, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (5): ReLU()\n",
       "  )\n",
       "  (edge_network): ModuleList(\n",
       "    (0-7): 8 x Sequential(\n",
       "      (0): Linear(in_features=768, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (5): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (node_network): ModuleList(\n",
       "    (0-7): 8 x Sequential(\n",
       "      (0): Linear(in_features=512, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (5): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (edge_decoder): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (5): ReLU()\n",
       "  )\n",
       "  (edge_output_transform): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=128, out_features=1, bias=True)\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = InteractionGNN2(hparams)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "504aabfb-86be-4b24-81aa-07db129e35eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "trainer = pl.Trainer(max_epochs=1)\n",
    "trainer.fit(model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936fb785-2125-4d5c-8176-ad117df52037",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.validate(model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc4ea00-4e5e-463e-ba56-1a788f629bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(model, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
